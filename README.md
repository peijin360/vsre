# VSREï¼šExpand VSR Benchmark for VLLM to Expertize in Spatial Rules
![vsre](https://github.com/user-attachments/assets/bc065667-c97c-4691-aae7-586c4decc42d)

Distinguishing spatial relations is a basic part of human cognition which requires fine-grained perception on cross-instance.  Although benchmarks like MME, MMBench and SEED  comprehensively have evaluated various capabilities which already include visual spatial reasoning(VSR). There is still a lack of sufficient quantity and quality evaluation and optimization datasets for Vision Large Language Models(VLLMs) specifically targeting visual positional reasoning. To handle this, we first diagnosed current VLLMs with the VSR dataset and proposed a unified test set.We found current VLLMs to exhibit a contradiction of over-sensitivity to language instructions and under-sensitivity to visual positional information.By expanding the original benchmark from two aspects of tunning data and model structure, we mitigated this phenomenon. 



## News
- 



---

## Table of Contents

- [Introduction](#introduction)
- [appendix material](https://github.com/user-attachments/files/18237160/appendix.pdf)



---

## Introduction


---


